from transformers import BertForSequenceClassification, TrainingArguments, Trainer
import datasets

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

for name, param in model.named_parameters():
    if 'classifier' not in name:  
        param.requires_grad = False

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,  
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="no",
    
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,  
    #eval_dataset=your_eval_dataset  # If you have a separate evaluation set
)

train_results = trainer.train()

print(train_results)